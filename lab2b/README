
INCLUDED FILES:
	lab2_list.c: source file that does updates to a linked list
	SortedList.h: header file provided by the project spec
	SortedList.c: implementation of the doubly linked list
	Makefile: supporting make build, tests, graphs, clean, dist, profile
	lab2_list.csv: includes comma-separated values from running
		lab2_list with different options by make tests
	-graphs generated by gnuplot (make graphs):
	lab2b_-1.png
	lab2b_2.png
	lab2b_3.png
	lab2b_4.png
	lab2b_5.png
	README: the current file
	lab2b_list.gp: gnuplot script for building the required graphs
	
RESOURCES:
	To figure out how to use gperf I used this https://github.com/
	gperftools/gperftools/wiki and https://gperftools.github.io/
	gperftools/cpuprofile.html to use CPUPROFILE. 
	I was getting LITTLE BENEFIT error for spin-lock-protected tests
	with 10 lists vs. 1 list; after a long time of debugging I couldn't
	find a problem with my code so I consulted a few classmates and 
	learned that I need to lock multiple times instead of just once.
	Previously I would only lock once before insert() and unlock after 
	the final delete(). I changed the implementation so that I lock/
	unlock 3 times (once for insert, once for length, and once for 
	lookup/delete) and that seemed to solve the problem.
	However, I still fail the sanity check about once every 6-7 times
	of running it with LITTLE BENEFIT errors. I assume this is due to 
	high traffic on server 9.
	GRAPHS: my graphs looked right before I implemented the three
	different lockings instead of one. I ran out of time to figure
	out what all the extra curves in my graphs are due to.

QUESTION 2.3.1:
	Most of CPU time for 1 and 2-thread list tests goes to inserting 
	elements in, looking them up and erasing them from the huge list.
	For example for the SortedList_lookup function we need to go thru
	the entire list for every single element. These are the most 
	expensive part of the code since we only have 1 or 2 threads so the
	thread creation and lock operations shouldn't take a very long time.
	In high-thread spin-lock tests most of the time goes to the spin-
	lock spinning while waiting to acquire the lock.
	In high-thread mutex tests most of the time is spent when waiting on
	a lock since many context switches need to happen and context 
	switches are expensive operations.

QUESTION 2.3.2:
	Around 84% of the CPU time is spent on the line containing the code 
	for locking the spin-lock. 
	This becomes expensive with a larger number of threads because each
	thread has to wait a longer iperiod of time to acquire the lock and
	 thus spins meanwhile.

QUESTION 2.3.3:
	Because each critical section can only be executed by one thread at
	a time. So increasing the number of threads increases the wait time 
	for each thread. 
	The completion time rises with increasing the number of threads 
	since each thread still has to wait a longer time to acquire a lock
	but this raise is not as dramatic since while a thread is waiting
	to acquire the lock, another one can be executing the critical
	section and getting work done.
	The wait time per operation can go up faster than the completion
	time per operation because the wait time includes the amount of time
	that all threads waited on a lock while the completion time doesn't
	include all the wait times.

QUESTION 2.3.4:
	Synchronized methods' performance gets better as we increase the 
	number of sublists. 
	We won't keep getting increased throughput with increasing the 
	number of lists because at some point each sublist ends up having
	very few elements in it and increasing the number of lists won't
	have an effect except for creating more list heads that might
	not even end up holding any elements.
	Seems like this trend does happen due to our data: throughput of 
	an N-way partitioned list is similar to that of a single list with
	1/N threads.
